1. I used two tests when it came to hardcoding examples for my Viterbi method. The first one is the method called fillMaps1. This one used the data provided in the problem set hidden Markov model example to fill in the transitions and observations. The next one, entitled fillMaps2 used an example model I created using the sentences "dogs are cool" and "he likes dogs". For the first test, I used simple sentences like "I will fish", and the Viterbi method worked fairly well in tagging these sentences. For my example, I typed in the sentences that created the example model, and these test sentences were correctly tagged. Through these two methods, I knew that the Viterbi method was functioning correctly.

2. One example of a sentence that is tagged correctly using the Brown documents is "She likes computers". My corresponding output was "She/N likes/VBZ computers/N". This short and simple sentence was correctly tagged. The sentence "I sing", however, produces the output "I/MOD sing/V". "I" should be a pronoun, but, likely because of the brevity of the sentence, there wasn't enough summation for the algorithm to work at its best. Regarding the accuracy of my algorithm, the console states: "The algorithm got 35037 out of 36394 correct using an unseen value of -100" when ran on the Brown test documents after being trained on the Brown files. This is fairly accurate, and near the accuracy rate given in the problem set. If I lower the unseen value to -1, I get the output "The algorithm got 2386 out of 36394 correct using an unseen value of -1". So, it's clear that a lower unseen value provides more accurate results.